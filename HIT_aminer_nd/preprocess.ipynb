{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:07:11.195075Z",
     "start_time": "2019-10-22T17:07:10.171002Z"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:08:09.782488Z",
     "start_time": "2019-10-22T17:07:11.199907Z"
    }
   },
   "outputs": [],
   "source": [
    "pub_dict = {}\n",
    "with codecs.open(\"data/train/train_pub.json\", \"r\", \"utf-8\") as f:\n",
    "    pub_dict = json.load(f)\n",
    "ad_pair = pd.read_csv(\"data/pair_data.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T06:11:52.935356Z",
     "start_time": "2019-10-22T06:11:52.911421Z"
    }
   },
   "outputs": [],
   "source": [
    "ad_pair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:08:13.678071Z",
     "start_time": "2019-10-22T17:08:09.791462Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_sample = ad_pair[ad_pair['2']==1][:1000]\n",
    "negative_sample = ad_pair[ad_pair['2']==0][:1000]\n",
    "sample_all = positive_sample.append(negative_sample)\n",
    "sample_all = sample_all.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:08:13.714989Z",
     "start_time": "2019-10-22T17:08:13.682059Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_sample[542:543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:08:13.737942Z",
     "start_time": "2019-10-22T17:08:13.720957Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_sample[542:543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:09:09.337015Z",
     "start_time": "2019-10-22T17:09:09.329036Z"
    }
   },
   "outputs": [],
   "source": [
    "pub_dict['vvpj0rbO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T07:52:50.624291Z",
     "start_time": "2019-10-22T07:52:50.195305Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from beard.similarity import AbsoluteDifference\n",
    "from beard.similarity import CosineSimilarity\n",
    "from beard.similarity.pairs import MyCosineSimilarity\n",
    "from beard.similarity.pairs import MyJaccardSimilarity\n",
    "from beard.similarity import JaccardSimilarity\n",
    "from beard.similarity import PairTransformer\n",
    "from beard.similarity import StringDistance\n",
    "from beard.similarity import EstimatorTransformer\n",
    "from beard.similarity import ElementMultiplication\n",
    "from beard.utils import FuncTransformer\n",
    "from beard.utils import Shaper\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T08:18:34.330648Z",
     "start_time": "2019-10-22T08:18:34.204951Z"
    }
   },
   "outputs": [],
   "source": [
    "def _build_distance_estimator(X, y, Xt, yt, verbose=0):\n",
    "    \"\"\"Build a vector reprensation of a pair of signatures.\"\"\"\n",
    "    transformer = FeatureUnion([\n",
    "        (\"author_name\", Pipeline([\n",
    "            (\"pairs\", PairTransformer(element_transformer=Pipeline([\n",
    "                (\"full_name\", FuncTransformer(func=get_authors)),\n",
    "                (\"shaper\", Shaper(newshape=(-1,))),\n",
    "                (\"tf-idf\", TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                                           ngram_range=(2, 4),\n",
    "                                           dtype=np.float32,\n",
    "                                           decode_error=\"replace\")),\n",
    "            ]))),\n",
    "            (\"combiner\", CosineSimilarity())\n",
    "        ])),\n",
    "        (\"affiliation_similarity\", Pipeline([\n",
    "            (\"pairs\", PairTransformer(element_transformer=Pipeline([\n",
    "                (\"affiliation\", FuncTransformer(func=get_author_affiliations)),\n",
    "                (\"shaper\", Shaper(newshape=(-1,))),\n",
    "                (\"tf-idf\", TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                                           ngram_range=(2, 4),\n",
    "                                           decode_error=\"replace\")),\n",
    "            ]))),\n",
    "            (\"combiner\", CosineSimilarity())\n",
    "        ])),\n",
    "        (\"title_similarity\", Pipeline([\n",
    "            (\"pairs\", PairTransformer(element_transformer=Pipeline([\n",
    "                (\"title\", FuncTransformer(func=get_title)),\n",
    "                (\"shaper\", Shaper(newshape=(-1,))),\n",
    "                (\"tf-idf\", TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                                           ngram_range=(2, 4),\n",
    "                                           dtype=np.float32,\n",
    "                                           decode_error=\"replace\")),\n",
    "            ]))),\n",
    "            (\"combiner\", CosineSimilarity())\n",
    "        ])),\n",
    "        # (\"journal_similarity\", Pipeline([\n",
    "        #     (\"pairs\", PairTransformer(element_transformer=Pipeline([\n",
    "        #         (\"journal\", FuncTransformer(func=get_journal)),\n",
    "        #         (\"shaper\", Shaper(newshape=(-1,))),\n",
    "        #         (\"tf-idf\", TfidfVectorizer(analyzer=\"char_wb\",\n",
    "        #                                    ngram_range=(2, 4),\n",
    "        #                                    dtype=np.float32,\n",
    "        #                                    decode_error=\"replace\")),\n",
    "        #     ]))),\n",
    "        #     (\"combiner\", CosineSimilarity())\n",
    "        # ])),\n",
    "        (\"venue_similarity\", Pipeline([\n",
    "            (\"pairs\", FuncTransformer(func=get_venue)),\n",
    "            (\"combiner\", MyJaccardSimilarity())\n",
    "        ])),\n",
    "        (\"abstract_similarity\", Pipeline([\n",
    "            (\"pairs\", PairTransformer(element_transformer=Pipeline([\n",
    "                (\"abstract\", FuncTransformer(func=get_abstract)),\n",
    "                (\"shaper\", Shaper(newshape=(-1,))),\n",
    "                (\"tf-idf\", TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                                           ngram_range=(2, 4),\n",
    "                                           decode_error=\"replace\")),\n",
    "            ]))),\n",
    "            (\"combiner\", CosineSimilarity())\n",
    "        ])),\n",
    "        (\"keywords_similarity\", Pipeline([\n",
    "            (\"pairs\", PairTransformer(element_transformer=Pipeline([\n",
    "                (\"keywords\", FuncTransformer(func=get_keywords)),\n",
    "                (\"shaper\", Shaper(newshape=(-1,))),\n",
    "                (\"tf-idf\", TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                                           ngram_range=(2, 4),\n",
    "                                           decode_error=\"replace\")),\n",
    "            ]))),\n",
    "            (\"combiner\", CosineSimilarity())\n",
    "        ])),\n",
    "        (\"year_diff\", Pipeline([\n",
    "            (\"pairs\", FuncTransformer(func=get_year, dtype=np.int)),\n",
    "            (\"combiner\", AbsoluteDifference())\n",
    "        ]))\n",
    "    ])\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=80,\n",
    "                                     max_depth=10,\n",
    "                                     max_features=7,\n",
    "                                     learning_rate=0.129,\n",
    "                                     verbose=verbose)\n",
    "    estimator = Pipeline([(\"transformer\", transformer),\n",
    "                          (\"clf\", clf)]).fit(X, y)\n",
    "    y_pred = estimator.predict(Xt)\n",
    "    print(\"\\tPrecision: %1.3f\" % precision_score(yt, y_pred))\n",
    "    print(\"\\tRecall: %1.3f\" % recall_score(yt, y_pred))\n",
    "    print(\"\\tF1: %1.3f\\n\" % f1_score(yt, y_pred))\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T07:52:54.909851Z",
     "start_time": "2019-10-22T07:52:54.875969Z"
    }
   },
   "outputs": [],
   "source": [
    "def learn_model(pub_dict, sample=True, verbose=0):\n",
    "    \"\"\"Learn the distance model for pairs of signatures.\n",
    "    \"\"\"\n",
    "    input_dataset = sample_all if sample else ad_pair\n",
    "    np.random.shuffle(input_dataset)\n",
    "    train, test = train_test_split(input_dataset, train_size=0.7)\n",
    "    X, y = train[:, :2], train[:, 2].astype(int)\n",
    "    Xt, yt = test[:, :2], test[:, 2].astype(int)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        X[i][0] = pub_dict[X[i][0]]\n",
    "        X[i][1] = pub_dict[X[i][1]]\n",
    "    for i in range(len(Xt)):\n",
    "        Xt[i][0] = pub_dict[Xt[i][0]]\n",
    "        Xt[i][1] = pub_dict[Xt[i][1]]\n",
    "    # Learn a distance estimator on paired signatures\n",
    "    distance_estimator = _build_distance_estimator(\n",
    "        X, y, Xt, yt, verbose=verbose)\n",
    "\n",
    "    pickle.dump(distance_estimator,\n",
    "                open(\"distance_model\", \"wb\"),\n",
    "                protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T08:20:04.370479Z",
     "start_time": "2019-10-22T08:20:00.008121Z"
    }
   },
   "outputs": [],
   "source": [
    "learn_model(pub_dict, sample=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T07:56:08.812608Z",
     "start_time": "2019-10-22T07:56:08.787713Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(sample_all)\n",
    "train, test = train_test_split(sample_all, train_size=0.7)\n",
    "X, y = train[:, :2], train[:, 2].astype(int)\n",
    "Xt, yt = test[:, :2], test[:, 2].astype(int)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i][0] = pub_dict[X[i][0]]\n",
    "    X[i][1] = pub_dict[X[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T14:42:36.088841Z",
     "start_time": "2019-10-22T14:42:36.063923Z"
    }
   },
   "outputs": [],
   "source": [
    "\" \".join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:05:32.880369Z",
     "start_time": "2019-10-22T17:05:32.856433Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_keywords(s):\n",
    "    res = \" \"\n",
    "    v = s[\"keywords\"] if 'keywords' in s and s['keywords'] is not None and len(s['keywords']) else ' '\n",
    "    if v[0] == '':\n",
    "        return res\n",
    "    if len(v):\n",
    "        res = \" \".join(v)\n",
    "    else:\n",
    "        res = \" \"\n",
    "    return res\n",
    "def get_abstract(s):\n",
    "    v = s[\"abstract\"] if 'abstract' in s and s['abstract'] is not None else ' '\n",
    "    if not v:\n",
    "        v = \" \"\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:08:52.816869Z",
     "start_time": "2019-10-22T17:08:52.802635Z"
    }
   },
   "outputs": [],
   "source": [
    "a = {'abstract': 'This paper discusses the current status of harmful algal blooms diagnosis and points out some of defects: time consuming and laborious. This paper described how to apply J2EE platform to forming an exact and efficient microscopic image diagnosis system of harmful algal blooms based on MVC model and by the technologies such as JSP, Servlets, EJB, JDBC etc.. ©2010 IEEE.',\n",
    " 'authors': [{'name': 'Liang Lv',\n",
    "   'org': 'Department of Electronic Engineering'},\n",
    "  {'name': 'Guangrong Ji', 'org': 'Department of Electronic Engineering'},\n",
    "  {'name': 'Chunfeng Guo', 'org': 'Department of Electronic Engineering'},\n",
    "  {'name': 'Xiang Gao', 'org': 'Department of Electronic Engineering'}],\n",
    " 'id': '5GnAKqKW',\n",
    " 'keywords': ['Harmful algal blooms',\n",
    "  'J2EE',\n",
    "  'Microscopic image',\n",
    "  'MVC',\n",
    "  'Red ride phytoplankton identification'],\n",
    " 'title': 'Design of microscopic image diagnosis system basedon MVC model and J2EE platform',\n",
    " 'venue': '2010 The 2nd International Conference on Computer and Automation Engineering, ICCAE 2010',\n",
    " 'year': 2010}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:08:55.248743Z",
     "start_time": "2019-10-22T17:08:55.239736Z"
    }
   },
   "outputs": [],
   "source": [
    "get_keywords({\"keywords\":['']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:08:58.022838Z",
     "start_time": "2019-10-22T17:08:58.015861Z"
    }
   },
   "outputs": [],
   "source": [
    "get_abstract({\"abstract\":''})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
